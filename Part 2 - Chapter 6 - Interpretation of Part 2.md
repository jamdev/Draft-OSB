#### CHAPTER 6

#### INTERPRETATION OF PART 2

**39 Meaning of “regulated content”, “user-generated content” and “news
publisher content”**


(1) This section applies for the purposes of this Part.


(2) “Regulated content”, in relation to a regulated user-to-user service, means
user-generated content, except—
(a) emails,
(b) SMS messages,
(c) MMS messages,
(d) comments and reviews on provider content (see subsection (5)),
(e) one-to-one live aural communications (see subsection (6)),
(f) paid-for advertisements (see subsection (7)), and
(g) news publisher content (see subsection (8)).


(3) “User-generated content”, in relation to a user-to-user service, means content—
(a) that is—
(i) generated by a user of the service, or
(ii) uploaded to or shared on the service by a user of the service,
and
(b) that may be encountered by another user, or other users, of the service
by means of the service.


(4) For the purposes of subsection (3)—
(a) the reference to content generated, uploaded or shared by a user
includes content generated, uploaded or shared by means of software
or an automated tool applied by the user;
(b) a bot is to be regarded as a user of a service if—
(i) the bot’s functions include interacting with user-generated
content, and
(ii) the bot is not operated by or on behalf of the provider of the
service.


(5) “Comments and reviews on provider content”, in relation to a user-to-user
service, means content present on the service consisting of comments on, or
reviews of, content produced and published on the service by the provider of
the service or by a person acting on behalf of the provider (together with any
further comments on such comments or reviews).


(6) “One-to-one live aural communications”, in relation to a user-to-user service,
means content consisting of communications made in real time between users
of the service by means of the service, if (and only if)—
(a) the communications consist solely of speech or other sounds conveyed
between two users,
(b) the communications do not include, and are not accompanied by, any
written message, video or other visual image, and
(c) the content is not a recording of such communications.


(7) An advertisement present on a user-to-user service or (as the case may be) in
search results is a “paid-for advertisement” if—
(a) the provider of the service receives any consideration (monetary or
non-monetary) for the advertisement (whether directly from the
advertiser or indirectly from another person), and
(b) the placement of the advertisement is determined by systems or
processes (human or automated) that are agreed between the parties
entering into the contract relating to the advertisement.


(8) “News publisher content”, in relation to a regulated user-to-user service,
means any content present on the service that is within subsection (9) or (10).


(9) Content is within this subsection if it was generated directly on the service by
a user of the service that is a recognised news publisher.


(10) Content is within this subsection if—
(a) the content was uploaded to or shared on the service by a user of the
service, and
(b) the content either—
(i) reproduces in full an article or written item that was originally
published by a recognised news publisher (and is not a
screenshot or photograph of that article or item or of part of it),
(ii) is a recording of an item originally broadcast by a recognised
news publisher (and is not an excerpt of such a recording), or
(iii) is a link to a full article or written item originally published by
a recognised news publisher, or to a full recording of an item
originally broadcast by a recognised news publisher.


(11) For the meaning of “recognised news publisher”, see section 40.


(12) The Secretary of State may by regulations repeal subsections (2)(d) and (5) if
the Secretary of State considers that it is appropriate to do so because of the risk
of harm to individuals in the United Kingdom presented by comments and
reviews on provider content.


(13) The Secretary of State may by regulations repeal subsections (2)(e) and (6) if the
Secretary of State considers that it is appropriate to do so because of the risk of
harm to individuals in the United Kingdom presented by one-to-one live aural
communications.


(14) In this section—
“MMS message” means a Multimedia Messaging Service message (that
may include images, sounds and short videos) that may be sent
between telephone numbers allocated in accordance with a national or
international numbering plan;
“SMS message” means a Short Message Service text message composed
principally of letters or numbers that may be sent between telephone
numbers allocated in accordance with a national or international
numbering plan.

**40 Meaning of “recognised news publisher”**


(1) In this Part, “recognised news publisher” means any of the following entities—
(a) the British Broadcasting Corporation,
(b) Sianel Pedwar Cymru,
(c) the holder of a licence under the Broadcasting Act 1990 or 1996 who
publishes news-related material in connection with the broadcasting
activities authorised under the licence, and
(d) any other entity which—
(i) meets all of the conditions in subsection (2), and
(ii) is not an excluded entity (see subsection (3)).


(2) The conditions referred to in subsection (1)(d)(i) are that the entity—
(a) has as its principal purpose the publication of news-related material,
and such material—
(i) is created by different persons, and
(ii) is subject to editorial control,
(b) publishes such material in the course of a business (whether or not
carried on with a view to profit),
(c) is subject to a standards code,

(d) has policies and procedures for handling and resolving complaints,
(e) has a registered office or other business address in the United
Kingdom,
(f) is the person with legal responsibility for material published by it in the
United Kingdom, and
(g) publishes—
(i) the entity’s name, the address mentioned in paragraph (e) and
the entity’s registered number (if any), and
(ii) the name and address of any person who controls the entity
(including, where such a person is an entity, the address of that
person’s registered or principal office and that person’s
registered number (if any)).


(3) An “excluded entity” is an entity—
(a) which is a proscribed organisation under the Terrorism Act 2000 (see
section 3 of that Act), or
(b) the purpose of which is to support a proscribed organisation under that
Act.


(4) For the purposes of subsection (2)—
(a) news-related material is “subject to editorial control” if there is a person
(whether or not the publisher of the material) who has editorial or
equivalent responsibility for the material, including responsibility for
how it is presented and the decision to publish it;
(b) “control” has the same meaning as it has in the Broadcasting Act 1990
by virtue of section 202 of that Act.


(5) In this section—
“news-related material” means material consisting of—
(a) news or information about current affairs,
(b) opinion about matters relating to the news or current affairs, or
(c) gossip about celebrities, other public figures or other persons in
the news;
“publish” means publish by any means (including by broadcasting), and
references to a publisher and publication are to be construed
accordingly;
“standards code” means—
(a) a code of standards that regulates the conduct of publishers,
that is published by an independent regulator, or
(b) a code of standards that regulates the conduct of the entity in
question, that is published by the entity itself.

**41 Meaning of “illegal content” etc**


(1) This section applies for the purposes of this Part.


(2) “Illegal content” means—
(a) in relation to a regulated user-to-user service, content—
(i) that is regulated content in relation to that service, and
(ii) that amounts to a relevant offence;
(b) in relation to a regulated search service, content that amounts to a
relevant offence.


(3) For the purposes of this section, content consisting of certain words, images,
speech or sounds amounts to a relevant offence if the provider of the service
has reasonable grounds to believe that—
(a) the use of the words, images, speech or sounds amounts to a relevant
offence,
(b) the use of the words, images, speech or sounds, when taken together
with other regulated content present on the service, amounts to a
relevant offence, or
(c) the dissemination of the content constitutes a relevant offence.


(4) “Relevant offence” means—
(a) a terrorism offence (see section 42),
(b) a CSEA offence (see section 43),
(c) an offence that is specified in, or is of a description specified in,
regulations made by the Secretary of State (see section 44), or
(d) an offence, not within paragraph (a), (b) or (c), of which the victim or
intended victim is an individual (or individuals).


(5) Illegal content—
(a) is “terrorism content” if the relevant offence is a terrorism offence;
(b) is “CSEA content” if the relevant offence is a CSEA offence;
(c) is “priority illegal content” if the relevant offence is an offence that is
specified in, or is of a description specified in, regulations under
subsection (4)(c).


(6) An offence is not to be regarded as a relevant offence within subsection (4)(d)
if the offence concerns—
(a) the infringement of intellectual property rights,
(b) the safety or quality of goods (as opposed to what kind of goods they
are), or
(c) the performance of a service by a person not qualified to perform it.


(7) For the purposes of determining whether content amounts to an offence (or to
an offence of a particular description), no account is to be taken of whether or
not anything done in relation to the content takes place in any part of the
United Kingdom.


(8) In relation to a regulated user-to-user service, the terms “illegal content”,
“terrorism content”, “CSEA content” and “priority illegal content” are to be
taken to include material which, if it were present on the service, would be
content within the definition in question (and this section is to be read with
such modifications as may be necessary for that purpose).


(9) In this section “offence” means an offence under the law of any part of the
United Kingdom.

**42 Offences relating to terrorism**


(1) In this Part “terrorism offence” means an offence specified in Schedule 2.


(2) The Secretary of State may by regulations amend Schedule 2.

**43 Offences relating to child sexual exploitation and abuse**


(1) In this Part “CSEA offence” means an offence specified in Schedule 3.


(2) The Secretary of State may by regulations amend Part 1 or 3 of Schedule 3.


(3) The Scottish Ministers may by regulations amend Part 2 of Schedule 3.

**44 Regulations under section 41**


(1) When deciding whether to specify an offence, or a description of offence, in
regulations under section 41, the Secretary of State must take into account—
(a) the prevalence on regulated services of content that amounts to that
offence or (as the case may be) offences of that description,
(b) the level of risk of harm being caused to individuals in the United
Kingdom by the presence of such content, and
(c) the severity of that harm.


(2) For the purposes of subsection (1), content consisting of certain words, images,
speech or sounds amounts to an offence or to an offence of a particular
description if there are reasonable grounds to believe that—
(a) the use of the words, images, speech or sounds amounts to the offence
or to an offence of that description,
(b) the use of the words, images, speech or sounds, when taken together
with other regulated content present on regulated services, amounts to
the offence or to an offence of that description, or
(c) the dissemination of the content constitutes the offence or an offence of
that description.


(3) For the purposes of determining whether content amounts to an offence (or to
an offence of a particular description), no account is to be taken of whether or
not anything done in relation to the content takes place in any part of the
United Kingdom.


(4) In subsection (1), the reference to the prevalence of content of a particular kind
is to be read, in relation to search services, as a reference to the prevalence of
content of that kind presented to users by operation of the search engine in
response to search requests made by them.


(5) The Secretary of State may not specify an offence in regulations under section
41 if the offence concerns—
(a) the infringement of intellectual property rights,
(b) the safety or quality of goods (as opposed to what kind of goods they
are), or
(c) the performance of a service by a person not qualified to perform it.


(6) The Secretary of State may not specify a description of offence in regulations
under section 41 if offences of that description are offences within subsection
(5).

**45 Meaning of “content that is harmful to children” etc**


(1) This section applies for the purposes of this Part.


(2) “Content that is harmful to children”, in relation to a regulated service, means
content that is—
(a) (in the case of a user-to-user service) regulated content in relation to
that service, and
(b) either—

(i) of a description designated in regulations made by the Secretary
of State as primary priority content that is harmful to children
(see section 47),
(ii) of a description designated in such regulations as priority
content that is harmful to children, or
(iii) within subsection (3) or (5).


(3) Content is within this subsection if the provider of the service has reasonable
grounds to believe that the nature of the content is such that there is a material
risk of the content having, or indirectly having, a significant adverse physical
or psychological impact on a child of ordinary sensibilities (“C”).


(4) For the purposes of subsection (3), in the case of content which may reasonably
be assumed to particularly affect people with a certain characteristic (or
combination of characteristics), or to particularly affect a certain group of
people, the provider is to assume that C possesses that characteristic (or
combination of characteristics), or is a member of that group (as the case may
be).


(5) Content is within this subsection if the provider of the service has reasonable
grounds to believe that there is a material risk of the fact of the content’s
dissemination having a significant adverse physical or psychological impact
on a child of ordinary sensibilities (“C”), taking into account (in particular)—
(a) how many users may be assumed to encounter the content by means of
the service, and
(b) how easily, quickly and widely content may be disseminated by means
of the service.


(6) For the purposes of subsections (3) and (5), the provider is to assess impact on
C by reference to children across the age-range, and the content is to be
regarded as within subsection (3) or (5) (as the case may be) if the provider has
reasonable grounds to believe that there is a material risk of impact as
mentioned in the relevant subsection on a child of any particular age.


(7) Where the provider has knowledge, relevant to the content, about a particular
child at whom content is directed, or who is the subject of it, subsections (3) and
(5) are to be read as if the reference to a child of ordinary sensibilities were a
reference to that particular child, taking into account any of the following
things that are known to or inferred by the provider—
(a) that child’s characteristics;
(b) that child’s membership of a certain group of people.


(8) The reference in subsection (3) to a risk of content “indirectly” having a
significant adverse physical or psychological impact on a child is a reference to
a risk of either of the following—
(a) content causing an individual to do or say things to a targeted child that
would have a significant adverse physical or psychological impact on
such a child;
(b) content causing a child to act in a way that—
(i) has a significant adverse physical or psychological impact on
that child, or
(ii) increases the likelihood of such an impact on that child.


(9) For the purposes of this section—
(a) illegal content (see section 41) is not to be regarded as within subsection
(3) or (5), and
(b) content is not to be regarded as within subsection (3) or (5) if the risk of
physical or psychological impact flows from—
(i) the content’s potential financial impact,
(ii) the safety or quality of goods featured in the content, or
(iii) the way in which a service featured in the content may be
performed (for example, in the case of the performance of a
service by a person not qualified to perform it).


(10) In this Part—
“non-designated content that is harmful to children” means content that
is harmful to children but that is not—
(a) primary priority content that is harmful to children, or
(b) priority content that is harmful to children;
“primary priority content that is harmful to children” means content of a
description designated as such in regulations under subsection (2)(b);
“priority content that is harmful to children” means content of a
description designated as such in regulations under subsection (2)(b).


(11) In relation to a regulated user-to-user service, the terms “content that is
harmful to children”, “primary priority content that is harmful to children”,
“priority content that is harmful to children” and “non-designated content that
is harmful to children” are to be taken to include material which, if it were
present on the service, would be content within the definition in question (and
this section is to be read with such modifications as may be necessary for that
purpose).


(12) In this section “targeted child”, in relation to content, means a child—
(a) who is the subject of the content, or
(b) who is a member of a class or group of people with a certain
characteristic (or combination of characteristics) targeted by the
content.

**46 Meaning of “content that is harmful to adults” etc**


(1) This section applies for the purposes of this Part.


(2) “Content that is harmful to adults”, in relation to a Category 1 service, means
content that is—
(a) regulated content in relation to that service, and
(b) either—
(i) of a description designated in regulations made by the Secretary
of State as priority content that is harmful to adults (see section
47), or
(ii) within subsection (3) or (5).


(3) Content is within this subsection if the provider of the service has reasonable
grounds to believe that the nature of the content is such that there is a material
risk of the content having, or indirectly having, a significant adverse physical
or psychological impact on an adult of ordinary sensibilities (“A”).


(4) For the purposes of subsection (3), in the case of content which may reasonably
be assumed to particularly affect people with a certain characteristic (or
combination of characteristics), or to particularly affect a certain group of
people, the provider is to assume that A possesses that characteristic (or

combination of characteristics), or is a member of that group (as the case may
be).


(5) Content is within this subsection if the provider of the service has reasonable
grounds to believe that there is a material risk of the fact of the content’s
dissemination having a significant adverse physical or psychological impact
on an adult of ordinary sensibilities, taking into account (in particular)—
(a) how many users may be assumed to encounter the content by means of
the service, and
(b) how easily, quickly and widely content may be disseminated by means
of the service.


(6) Where the provider has knowledge, relevant to the content, about a particular
adult at whom content is directed, or who is the subject of it, subsections (3)
and (5) are to be read as if the reference to an adult of ordinary sensibilities
were a reference to that particular adult, taking into account any of the
following things that are known to or inferred by the provider—
(a) that adult’s characteristics;
(b) that adult’s membership of a certain group of people.


(7) The reference in subsection (3) to a risk of content “indirectly” having a
significant adverse physical or psychological impact on an adult is a reference
to a risk of either of the following—
(a) content causing an individual to do or say things to a targeted adult
that would have a significant adverse physical or psychological impact
on such an adult;
(b) content causing an adult to act in a way that—
(i) has a significant adverse physical or psychological impact on
that adult, or
(ii) increases the likelihood of such an impact on that adult.


(8) For the purposes of this section—
(a) illegal content (see section 41) is not to be regarded as within subsection
(3) or (5), and
(b) content is not to be regarded as within subsection (3) or (5) if the risk of
physical or psychological impact flows from—
(i) the content’s potential financial impact,
(ii) the safety or quality of goods featured in the content, or
(iii) the way in which a service featured in the content may be
performed (for example, in the case of the performance of a
service by a person not qualified to perform it).


(9) “Priority content that is harmful to adults” means content of a description
designated as such in regulations under subsection (2)(b).


(10) In relation to a Category 1 service, the terms “content that is harmful to adults”
and “priority content that is harmful to adults” are to be taken to include
material which, if it were present on the service, would be content within the
definition in question (and this section is to be read with such modifications as
may be necessary for that purpose).


(11) In this section “targeted adult”, in relation to content, means an adult—
(a) who is the subject of the content, or

(b) who is a member of a class or group of people with a certain
characteristic (or combination of characteristics) targeted by the
content.

**47 Regulations under sections 45 and 46**


(1) In this section “regulations” means—
(a) regulations under section 45 (designation of descriptions of content as
primary priority content that is harmful to children or priority content
that is harmful to children), or
(b) regulations under section 46 (designation of descriptions of content as
priority content that is harmful to adults).


(2) The Secretary of State must consult OFCOM before making regulations.


(3) For so long as regulations are in force, OFCOM must carry out reviews of—
(a) the incidence on regulated services of content that is harmful to
children and content that is harmful to adults, and
(b) the severity of harm that individuals in the United Kingdom suffer, or
may suffer, as a result of those kinds of content.


(4) OFCOM must prepare and publish a report on the outcome of each review.


(5) The report must contain—
(a) OFCOM’s conclusions about whether provision made by the
regulations remains appropriate, and
(b) in consequence of those conclusions, OFCOM’s recommendations in
relation to the exercise of the Secretary of State’s power to make
regulations.


(6) The reports must be published not more than three years apart.


(7) The first report must be published before the end of the period of three years
beginning with the day on which the first statutory instrument containing
regulations is made.


(8) On completion of each report, OFCOM must, as soon as reasonably
practicable—
(a) send it to the Secretary of State, and
(b) publish it in such manner as OFCOM consider appropriate for bringing
it to the attention of persons who, in their opinion, are likely to be
affected by it.


(9) In subsection (3)(a), the reference to the incidence of content of a particular
kind is to be read, in relation to search services, as a reference to content of that
kind presented to users by operation of the search engine in response to search
requests made by them.

**48 Meaning of “Chapter 2 safety duty” and “Chapter 3 safety duty”**


(1) In this Part “Chapter 2 safety duty” means any of the duties set out in—
(a) section 9 (safety duties about illegal content),
(b) section 10 (safety duties for services likely to be accessed by children),
or
(c) section 11 (safety duties protecting adults).
(2) In this Part “Chapter 3 safety duty” means any of the duties set out in—
(a) section 21 (safety duties about illegal content), or
(b) section 22 (safety duties for services likely to be accessed by children).


#### EXPLANATORY NOTES FOR THIS SECTION

Chapter 6: Interpretation of Part

Clause 39: Meaning of “regulated content”, “user-generated content” and “news publishers
content”

This clause defines “regulated content” and “user-generated content”. It
establishes that “regulated content” is a subset of “user-generated content”, and
describes the types of user-generated content which do not count as “regulated
content”.
Subsection (2) states that “regulated content” on user-to-user services is
user-generated content, with the exception of the types of content listed in
paragraphs (a) to (g).
The safety duties imposed on the provider of a regulated service will not
cover the types of user-generated content which are exempt from the definition of
“regulated content”. For example, a social media service will be a regulated service,
but the provider of the service will not have duties with regards to the types of user-
generated content listed in paragraphs (a)-(g) of subsection (2). This differs from the
exemptions established in clause 3 and Schedule 1, which exempt whole types of
services from the scope of the regulatory framework.
Subsection (3) defines “user-generated content” as content that is generated
by a user of the service, or uploaded to or shared on the service by a user of the
service, and which may be encountered by another user (or users) by means of the
service. For example, this would include anything posted on a forum or social media
network; a direct message from one user to another; or a file stored in shared cloud
storage.
“User-generated content” does not include content published by the service
provider. For example, a blog posted by someone on their own site, a press release
posted by a company on its own site, or a television programme published by a
broadcaster on its own streaming app would not count as user-generated content.

Furthermore, “user-generated content” does not include content uploaded by
a user which may not be encountered by any other users. For example, content
shared only between a user and the service provider (such as through a customer
service chat function) would not fall under the definition of user-generated content.
Subsection (4) provides additional detail for the purposes of subsection (3).
Subsection (4)(a) establishes that content which is generated, uploaded or shared by
means of software or an automated tool applied by the user counts as “user-
generated content”. For example, if a user used a tool for content to be uploaded
automatically at a later date, the content would still count as user-generated content.
Equally, if a user uploaded content to Service A, and applied a tool which
automatically also shared it to Service B, the content would count as user-generated
content on both Service A and Service B.
Subsection (4)(b) establishes that for these purposes a bot counts as a user if
it interacts with user-generated content, as long as it is not operated by or on behalf
of the provider. For example, a bot unconnected with the provider of a service which
posted content on that service would count as a user (and the content would
therefore count as “user-generated content”). However, a bot operated by the service
provider posting updates to the provider’s service, would not count as a user of that
service, and therefore the content posted would not count as “user-generated
content”.
Subsection (5) defines “comments and reviews on provider content” in
respect of user-to-user services; this relates to the exemption under subsection
(2)(d). This definition encompasses user comments and reviews in relation to all
content that is directly uploaded by a service provider. In practice, this exemption will
primarily benefit ‘below the line’ comments on articles on news publisher’s sites and
user reviews on directly advertised products and services. This exemption will not
apply to comments on or reviews of user-generated content such as goods and
services advertised by third party sellers on online marketplaces.
Subsection (6) defines “one-to-one live aural communications”; this relates to
the exemption in subsection (2)(e). It makes clear that only aural communications
between two users which are not accompanied by written messages, videos or other
visual images, and which are not recordings of such content, are exempt from the
definition of “regulated content”. For example, a one-to-one live voice call over an
internet service would not count as “regulated content”, but a one-to-one video call or
a recording of a call shared on a regulated service would.
Subsection (7) defines “paid-for advertisements”; this relates to the exemption
in subsection (2)(f). To count as a “paid-for advertisement”, firstly the service provider
must receive “consideration” of any sort (such as money, a gift, or data) for the
advertisement. That consideration may come directly from the advertising party, or
may come indirectly, for example through an intermediary company. Secondly, the
advertisement must be placed by systems or processes that are agreed between the
contracting parties. Such systems and processes may be human (for example, an
agreement to place an advert in a particularly prominent place on a service) or
automated (for example, through an algorithm which tailors adverts to specific types
of users).
Subsections (8) to (10) define “news publisher content”; this relates to the
exemption in subsection (2)(g). Under subsection (9), content on a service directly
generated by a recognised news publisher (as defined in clause 40) does not count
as user-generated content. Under subsection (10), content originally published by a
recognised news publisher but uploaded to or shared on a service by another user of
that service, either in its entirety or by way of a link to the entirety of the material, also
does not count as user-generated content. For example, if a user shares the text of
an article copied from a recognised news publisher’s website, with no additions or
amendments, that text will not count as user-generated content. If content generated
by a recognised news publisher that has been amended by another user is uploaded
to or shared on the service by another user it will count as user-generated content.
For example, if a user shares only part of an article, the extract shared will count as
user-generated content. Any commentary about the article shared at the same time
as the article also counts as user-generated content. Equally, if an image of content
generated by a recognised news publisher is uploaded to or shared on a service by
another user that image will also count as user-generated content.
Subsection (12) confers a power on the Secretary of State to repeal by
regulations the exemption for comments and reviews on provider content, if the
Secretary of State considers that it is appropriate to do so because of the risk of
harm to individuals presented by this type of user-generated content. Subsection (13)
confers an equivalent power in relation to one-to-one live aural communications. In
both cases, regulations would be subject to the affirmative procedure.
Clause 40: Meaning of “recognised news publisher”

This clause defines the term “recognised news publisher”.
Subsection (1) states that the British Broadcasting Corporation, Sianel
Pedwar Cymru, and any entity that holds a license under the Broadcasting Act 1990
or 1996 and which publishes news-related content under that license will qualify as a
recognised news publisher. Subsection (1)(d) adds that any entity that meets the
conditions listed in subsection (2) will also be considered a “recognised news
publisher”, providing it is not excluded under subsection (3).
Subsection (2) then sets out the conditions that other entities have to meet in
order to be considered a “recognised news publisher” under subsection (1)(d).
Subsection (3) sets out the conditions in which an entity is excluded from the
definition of “recognised news publisher”, even where it otherwise meets the criteria
set out in subsection (2). These are that the entity is a proscribed organisation under
the Terrorism Act 2000 or is an entity which supports such an organisation.
Paragraph (a) of subsection (4) defines the conditions in which news-related
material can be said to be “subject to editorial control”. This relates to the conditions
for a “recognised news publisher” in subsection (2). The term “control” is defined in
paragraph (b) by reference to section 202 of the Broadcasting Act 1990, which itself
refers to the detailed provisions in paragraph 1(1) of Part 1 of Schedule 2 to that Act.
Subsection (5) defines the terms “news-related material”, “publisher” and
“standards code”.
Clause 41: Meaning of “illegal content”

This clause defines illegal content using the concept of content which
amounts to a relevant offence (subsections (2) and (3)). Content amounts to a
relevant offence if the provider of the service has reasonable grounds to believe that
the use of the words, images, speech or sounds in the content (either in itself or
when taken together with other content present on the service) amounts to a relevant
offence, or that the dissemination of the content constitutes a relevant offence.
Subsection (4) defines a “relevant offence” as a terrorism offence (see clause
42), a child sexual exploitation and abuse (CSEA) offence (see clause 43), an
offence which the Secretary of State has specified in regulations under clauses 41
and 43, or another offence which has or is intended to have one or more individual
victims. Only offences under the law of any part of the United Kingdom can be
relevant offences (subsection (9). Examples of relevant offences that are not
terrorism or CSEA offences include revenge pornography, the sale of illegal goods,
and upskirting.
Subsection (5) clarifies that when the relevant offence is a terrorism offence
or a CSEA offence, the content is described as terrorism content or CSEA content
respectively. This subsection also clarifies that when the relevant offence is specified
in regulations it is described as priority illegal content.
Subsection (6) excludes certain categories of offence from the definition of
illegal content. These are offences involving infringements of intellectual property
rights, unsafe or substandard goods and services provided by an unqualified
provider.
To reflect the difficulty of determining where online content is generated,
subsection (7) provides that, when determining whether content amounts to an
offence, no account is to be taken of whether or not anything done in relation to that
content takes place in the United Kingdom.
Subsection (8) clarifies that illegal content, terrorism content, CSEA content
and priority illegal content does not have to be present on a regulated service to meet
the relevant definitions for these types of content. This provision is necessary to allow
service providers to take steps in relation to content which would not otherwise meet
the definitions because it had not yet been uploaded to or shared on their services.
Clause 42: Offences relating to terrorism

This clause refers to Schedule 2 to the Bill, which defines the offences that
constitute “terrorism content”.
They are based in existing domestic legislation, including from the Terrorism
Act 2000 and 2006, and the Anti-terrorism, Crime and Security Act 2001. The Bill
does not introduce any new offences.
The Secretary of State can amend Schedule 2 through regulations.
Schedule 2

Schedule 2 defines the offences that constitute “terrorism content”.
Paragraph 1 lists the relevant provisions that apply from the Terrorism Act
2000, and Paragraph 3 lists the relevant provisions from the Terrorism Act 2006
Paragraph 2 sets out the relevant section from the Anti-terrorism, Crime and
Security Act 2001.
Paragraph 4 sets out, for the purpose of this Schedule, an inchoate offence
as:
a. An offence of attempting to, or conspiring to, commit any of the offences set
out above.
b. An offence under Part 2 of the Serious Crime Act 2007 in relation to any of
the offences set out above. In Scotland, this would also include inciting a
person to commit such offence.
c. An office of aiding, abetting, counselling or procuring the commission of an
offence set out above. In Scotland, this includes being involved, art and part,
in commissioning of such an offence.
Clause 43: Offences relating to child sexual exploitation and abuse

This clause refers to Schedule 3 to the Bill, which defines the child sexual
exploitation and abuse (CSEA) offences that apply to this Bill.
They are based on existing legislation for CSEA offences which applies in
England and Wales, Scotland and Northern Ireland. The Bill does not introduce any
new offences.
The Secretary of State can amend Part 1 or 3 of Schedule 3 and Scottish
Ministers can amend Part 2 of Schedule 3 through regulations.
Schedule 3

Schedule 3 defines the child sexual exploitation and abuse offences that
apply to this Bill. Part 1 applies to England and Wales, Part 2 applies to Scotland,
and Part 3 applies to Northern Ireland.
Clause 44: Regulations under section 41

This clause sets out the matters to be taken into account by the Secretary of
State when making regulations specifying relevant offences for the purpose of
defining illegal content.
Subsection (1) requires the Secretary of State to take account of the
prevalence of the offence on regulated services, the risk of harm to individuals in the
UK and how severe that harm is likely to be.
Subsections (2) to (6) make equivalent provision in relation to offences as is
made in subsections (3), (6) and (7) of clause 41.
Clause 45: Meaning of “content that is harmful to children” etc

This clause defines “content that is harmful to children”.
Subsection (2) defines “content that is harmful to children”. It defines this as
regulated content (see clause 39) which the Secretary of State has designated in
regulations (see clause 47) as either primary priority or priority content which is
harmful to children, or which meets the conditions of subsections (4) or (6) of this
clause.
Subsection (3) states that content is in scope of regulation if the provider has
reasonable grounds to believe that the nature of the content risks directly or indirectly
having a significant adverse physical or psychological impact on a child of ordinary
sensibilities. This could be by indirectly resulting in physical injuries or by directly or
indirectly resulting in a significant negative effect on the mental state of an individual.
This could include causing feelings such as serious anxiety and fear; longer-term
conditions such as depression and stress; and medically recognised mental
illnesses, both short-term and permanent.
Subsection (4) provides that, where harmful content would reasonably be
assumed to particularly affect people with certain characteristics or belonging to a
certain group, for example people with disabilities or people of a particular religion,
the provider should assume that the child encountering that content possesses those
characteristics or belongs to that group.
Subsection (5) provides that content may be harmful to children due to the
way in which it is disseminated, even if the nature of the content is not itself harmful,
for example, repeatedly sending apparently innocuous content to a user could be
bullying or intimidating. In determining whether content is harmful to children, a
provider should also take into account how many users could be encountering the
content on the service and how easily, quickly and widely the content can be
disseminated on the service.
Subsection (6) clarifies that for the purposes of (3) and (5), the provider
should assess the impact on a child of ordinary sensibilities with reference to children
across the age range. Content is to be regarded in scope of (3) and (5) if the provider
has reasonable grounds to believe that there is a risk of harm to children of any
particular age.
These explanatory notes relate to the Online Safety Bill as published in draft on 12 May 2021 (Bill CP 405)
Subsection (7) provides that where the provider has knowledge about the
child that content in scope of subsections (3) and (5) is directed at, or if the child is
the subject of the content, subsections (3) to (5) should be read in reference to that
particular child instead of a child of ordinary sensibilities, taking into account that
child’s characteristics or membership of a certain group of people, where known or
inferred by the provider.
Subsection (8) sets out that content could ‘indirectly’ cause harm as
described in subsection (3) where it causes an individual to do or say things to a
targeted child that would have a significant adverse physical or psychological impact
on that child, or where it causes a child to act in a way which has, or increases the
likelihood, of a significant adverse physical or psychological impact on that child (for
example, content which promotes risky or violent behaviour).
Subsection (9) sets out content that should not be regarded as within
subsections (3) or (5), namely illegal content or content where the risk of physical or
psychological harm comes from the content’s potential financial impact, the safety or
quality of goods featured in the content, or the way in which a service featured in the
content may be performed.
Subsections (10) - (12) set out definitions for the purposes of this clause.
Clause 46: Meaning of “content that is harmful to adults” etc

This clause defines “content that is harmful to adults”.
Subsection (2) defines “content that is harmful to adults” on Category 1
services. It defines this as regulated content on a service (see clause 39), which is
either priority content, designated as such by the Secretary of State in regulations
(see clause 47), or which meets the conditions of subsections (3) and (5).
The remainder of this clause contains equivalent provisions to those in
subsections (3) to (5) and (7) to (12) of clause 45.
Clause 47: Regulations under sections 45 and 46

This clause makes provision in relation to the making of regulations under
clauses 45 and 46 designating primary priority and priority content that is harmful to
children, and priority content that is harmful to adults. It also sets out requirements for
OFCOM to review and report on their efficacy.
Clause 48: Meaning of “Chapter 2 safety duty” and “Chapter 3 safety duty”

This clause signposts the sections relevant to the key terms used in Part 2 of
the Bill.

#### DELEGATED POWERS MEMORANDUM

CHAPTER 6: INTERPRETATION OF PART 2
Clause 39(12) and (13): Meaning of “regulated content”, “user - generated content”
and “news publisher content”

Powers conferred on: Secretary of State
Powers exercised by: Regulations
Parliamentary procedure: Affirmative

Context and purpose

This clause defines various terms which determine what type of content is in scope of
regulation. It defines “user-generated content” (subsections (3) and (4)) and
establishes “regulated content” as a subset of “user-generated content”. Subsection
(2) provides that “regulated content” is all “user-generated content” except for the
following:
○ Emails;
○ SMS/MMS messages;
○ Comments and reviews on provider content;
○ One-to-one live aural communications;
○ Paid-for advertisements; and
○ News publisher content.
This means that the duties imposed on providers of regulated user-to-user services
will not apply in respect of these types of content.
Clause 39(12) and (13) enable the Secretary of State to make regulations to repeal
the exemptions for some types of user-generated content (comments and reviews on
provider content, and one-to-one live aural communications). Repealing these
exemptions would bring that content into scope of regulation. The powers can only be
exercised if the Secretary of State considers it appropriate because of the risk of
harm to individuals in the United Kingdom. For example, the Secretary of State may
repeal the exemption if evidence emerges of an increased risk of harm from such
types of content.
The Department has worked to identify areas where future changes in user behaviour
could necessitate the repeal of exemptions of specific types of content. It is creating
this limited and targeted power to repeal those exemptions. Specifically, the
Department considers it appropriate for the power to repeal exemptions to apply only
to comments and reviews on provider content, and to one-to-one live aural
communications, as it has determined these types of user-generated content to be
the most likely to lead to an increased risk of future harm due to the dynamic nature
of these functionalities.
These are Henry VIII powers in that they allow the Secretary of State to repeal
specified provisions of the Bill using secondary legislation.
Justification for the powers

Comments and reviews on provider content and one-to-one live aural
communications will be exempt from regulation due to the low risk of harm which they
pose to individuals in the United Kingdom. Exempting them from scope ensures that
businesses will not be subject to disproportionate regulatory burdens.
However, for the regulatory framework to function effectively, it must remain
responsive to technological changes. Types of content which are currently low-risk,
and which therefore merit a complete exemption from the framework today, may pose
a higher risk of harm in the future. This is particularly important as harm can migrate
quickly across different types of content. It is important that, should the level of risk
from exempt types of content rise to the point where an exemption is no longer
merited, those types of content can be quickly brought into scope of the regulatory
framework.
Importantly, the power can only be exercised if the Secretary of State considers that it
is appropriate to do so because of the risk of harm to individuals in the United
Kingdom presented by the type of content in question.
Justification for the procedure

By virtue of clause 132(4)(c), regulations made under clause 39(12) and (13) are
subject to the affirmative procedure.
The affirmative procedure is considered appropriate as any exemptions would place
new regulatory burdens on businesses providing regulated services which include the
previously exempt types of content, so it is important that Parliament has the
opportunity to debate any such change.
Clause 41(4): Meaning of “illegal content” etc.

Power conferred on: Secretary of State
Power exercised by: Regulations
Parliamentary procedure: First time - affirmative procedure. Negative procedure
thereafter.

Context and purpose

The regulatory framework places duties on all regulated providers to take steps in
relation to illegal content on their services (see clauses 9 and 21).
Clause 41 defines “illegal content” by reference to content which the provider of the
service has reasonable grounds to believe amounts to a relevant offence or whose
dissemination amounts to a relevant offence (see subsections (2) and (3) of clause
41).
Subsection (4) provides that a “relevant offence” includes a terrorism offence (see
clause 42 and Schedule 2) or a child sexual exploitation and abuse offence (see
clause 43 and Schedule 3), an offence specified by the Secretary of State in
regulations, and other offences where the victim or intended victim is an individual
(subject to certain exceptions set out in subsection (6)). Any content relating to an
offence specified by the Secretary of State in such regulations is “priority illegal
content” (see subsection (5)(c)). The safety duties about illegal content set out in
clause 9 impose duties on providers of user-to-user services in relation to the
minimisation of the presence and dissemination of priority illegal content (see
subsection (3) of that clause) in addition to those imposed in relation to other legal
content.
The Secretary of State is thus given a power by clause 41 to specify in regulations
what offences (which are not otherwise specified terrorism or child sexual exploitation
and abuse offences) amount to a relevant offence and so whether content relating to
such an offence is to be “priority illegal content” or not. Clause 44(1) provides that
when specifying offences in exercise of this power the Secretary of State must take
into account the prevalence of relevant content on regulated services, the level of risk
of harm being caused to individuals in the United Kingdom and how severely they
might be harmed.
Justification for the power

This delegated power is considered necessary to allow the legislation to be updated
to address changes in the criminal law and emerging types of illegal content online, to
protect individuals in the United Kingdom from the significant adverse physical and
psychological impacts that such content can cause.
Justification for the procedure

By virtue of clause 132(6)(a), on the first occasion of its use regulations made under
clause 41 must be approved by Parliament through the affirmative procedure. The
first use of this power will determine which offences providers of regulated services
must prioritise, which will be a central element of the new regulatory framework. It is
therefore considered appropriate that Parliament should be able to debate and
approve the original regulations made under this power.
By virtue of clause 132(5)(a), in all subsequent cases the power will be exercised
through the negative procedure. The negative procedure is considered to provide an
appropriate level of parliamentary scrutiny for regulations made under this power
which will simply specify new offences or remove others from the list. It will also allow
the government to respond quickly to the emergence of significant new types of illegal
content.
Clause 42(2): Offences relating to terrorism

Power conferred on: Secretary of State.
Power exercised by: Regulations
Parliamentary procedure: Affirmative

Context and purpose

Under the safety duties about illegal content in clauses 9 and 21, providers of
regulated services are subject to obligations in relation to priority illegal content and
illegal content. The reporting and redress duties in clauses 15 and 24 also impose
obligations on providers in relation to making users aware of how to flag and report
such content and their policies for dealing with it.
Schedule 2 lists existing offences under United Kingdom terrorism legislation that are
capable of being committed online, wholly or in part. Under clause 42(2), the
Secretary of State will be able to amend Schedule 2 and therefore this is a Henry VIII
power.
Justification for the power

The Online Safety framework needs to be able to adapt to new terrorist offences.
The United Kingdom terrorism legislation landscape is likely to change in future as
new offences are introduced to deal with the evolving terrorist threat.
In addition, changes in terrorist behaviour, or to the ways in which online services are
used to facilitate terrorist behaviour, could cause existing offences that government
does not currently consider as online terrorist offences to move into this space.
Delegating this power to make regulations is essential in ensuring that the framework
can respond and adapt to changes in terrorist behaviour and keep users safe.
Justification for the procedure

By virtue of clause 132(4)(d), regulations made under clause 42(2) are subject to the
affirmative procedure. The affirmative procedure ensures that any amendments to the
list of terrorism offences in Schedule 2 are fully debated and scrutinised in
Parliament. The affirmative procedure is therefore considered to provide an
appropriate level of parliamentary scrutiny for regulations made under this power.
.
Clause 43(2): Offences relating to child sexual exploitation and abuse

Power conferred on: Secretary of State
Power exercised by: Regulations
Parliamentary procedure: Affirmative

Context and purpose

Under the safety duties about illegal content in clauses 9 and 21, providers of
regulated services are subject to obligations in relation to priority illegal content and
illegal content. The reporting and redress duties in clauses 15 and 24 also impose
obligations on providers in relation to making users aware of how to flag and report
such content and their policies for dealing with it.
To ensure that the framework is proportionate and to provide companies and the
regulator with legal certainty, Schedule 3 defines which offences are CSEA content
for the purposes of the legislation. Schedule 3 is split into Parts to reflect that the
criminal law in this area is devolved. Part 1 covers CSEA legislation in England and
Wales, Part 2 does the same for Scotland and Part 3 for Northern Ireland.
Under clause 43(2) the Secretary of State will be able to amend Part 1 and Part 3 of
Schedule 3. This is a Henry VIII power in that it allows the Secretary of State to
amend provisions of the Bill.
This power will ensure that the framework, and the duties it imposes on companies to
keep children safe, remain up to date with technological change and legislative
developments.
Justification for the power

The Online Safety framework needs to be able to adapt to new CSEA offences. Part
1 of Schedule 3 lists existing CSEA offences in England and Wales that are capable
of being committed online, wholly or in part. Part 3 of Schedule 3 similarly lists
existing CSEA offences in Northern Ireland that are capable of being committed
online, wholly or in part. Additional CSEA offences within England and Wales and
within Northern Ireland are likely to be created to respond to new and evolving threats
and in response to new technologies.
Delegating this power will enable the list of CSEA offences to be updated to include
relevant new offences, and so help to keep children safe by ensuring that the
framework can respond and adapt to changes in CSEA offences.
The Secretary of State is the appropriate person to exercise the power to amend Part
1 because that part relates to criminal law in England and Wales. With regards to the
power to amend Part 3, while the devolution settlement provides that the Northern
Ireland Assembly can make regulations dealing with internet services, such
regulations require the consent of the Secretary of State. Therefore, the Secretary of
State is assessed to be the appropriate person to exercise this power.
There are clear limitations on the power. The inclusion of the Schedule in primary
legislation ensures that its contents will be subject to scrutiny and debate in
Parliament. While this power allows the use of secondary legislation to amend the list
of offences in the Schedule, it only allows Parts 1 and 3 of Schedule 3 to be updated
to include new criminal offences or to remove existing ones related to child abuse and
exploitation.
Justification for the procedure

By virtue of clause 132(4)(e), regulations made under clause 43(2) are subject to the
affirmative procedure. The affirmative procedure ensures that any amendments to the
Schedule are fully debated and scrutinised in Parliament.
This procedure is therefore considered to provide an appropriate level of
parliamentary scrutiny for regulations made under this power.
Clause 43(3): Offences relating to child sexual exploitation and abuse

Power conferred on: Scottish Ministers
Power exercised by: Regulations
Parliamentary procedure: Affirmative

Context and purpose

Protecting children is central to the online safety regulatory framework. To ensure that
the framework is proportionate and to provide companies and the regulator with legal
certainty, Schedule 3 defines what offences are defined as CSEA content for the
purposes of the legislation. Criminal law is devolved under the Scotland Act 1998 so
Part 2 of Schedule 3 specifies which offences under Scottish law are defined as
CSEA content for the purposes of the legislation.
Under clause 43(3), Scottish Ministers will be able to amend Part 2 of Schedule 3.
This power will ensure that the regulatory framework, and the duties it imposes on
companies to keep children safe, remains up to date with technological change and
legislative developments. This is a Henry VIII power in that it allows secondary
legislation to be used to amend provisions in the Bill.
Justification for the power

The Online Safety framework needs to be able to adapt to new CSEA offences. Part
2 of Schedule 3 lists existing CSEA offences in Scotland that are capable of being
committed online, wholly or in part. Additional CSEA offences within Scotland are
likely to be created to respond to new and evolving threats and in response to new
technologies.
Delegating this power will enable the Act to include relevant new CSEA offences in
Scotland and keep children safe by ensuring that the framework can respond and
adapt to changes in CSEA-related activity online.
The Scottish Ministers are the appropriate persons to exercise this power because it
relates to Scottish criminal law.
There are clear limitations on the power. The inclusion of the Schedule in primary
legislation ensures that it will be subject to scrutiny and debate in Parliament. While
this power allows the use of secondary legislation to amend Part 2 of Schedule 3, it
will only allow the framework to be updated to include new CSEA offences or to
remove existing ones related to child abuse and exploitation in Scotland.
Justification for the procedure

By virtue of clause 132(7), regulations made under section are subject to the
affirmative procedure. The affirmative procedure ensures that any amendments are
fully debated and scrutinised in the Scottish Parliament.
This procedure is therefore considered to provide an appropriate level of
parliamentary scrutiny for regulations made under this power.
Clause 45(2): Meaning of “content that is harmful to children” etc

Power conferred on: Secretary of State
Power exercised by: Regulations
Parliamentary procedure: First time - affirmative procedure. Negative procedure
thereafter.

Context and purpose

Providers of regulated services which are assessed as likely to be accessed by
children, in accordance with clause 26, will have to carry out a children’s risk
assessment under clause 7(2) to identify content which is legal but harmful to
children, and to take steps under clause 10 to ensure adequate systems and
processes are in place to protect children from them and mitigate the risk of harm.
Clause 45 defines the meaning of 'content that is harmful to children' and confers on
the Secretary of State a power, under subsection (2)(b), to specify in regulations both
“primary priority” and “priority” content which is harmful to children. Service providers
will have to consider each of these priority categories of content in their risk
assessment.
Justification for the power

This power is delegated to reflect the need to ensure that the regime is future-
proofed, but also to provide legal certainty. There will be parliamentary oversight of
the definition of harm in the primary legislation, and then parliamentary oversight of
the specific categories of “primary priority” and “priority” content that is harmful to
children as part of the scrutiny of secondary legislation made under this power.
This will allow high level parliamentary oversight of the definition of content which is
harmful to children, allowing new types of content to be added in response to
changes in risk or technologies so as to ensure that they are effectively dealt with
under the regulatory framework.
This delegated power will further ensure that the regulatory framework benefits fully
from OFCOM's expertise and research capacity. Under clause 47, the Secretary of
State must consult OFCOM before making regulations under this power. Within three
years of the date of the first use of either the clause 45 or clause 46 power, OFCOM
must prepare a report focusing on the incidence on regulated services of content that
is harmful to children and content that is harmful to adults, and the severity of harm
that individuals suffer, or may suffer, as a result of those kinds of content. The report
must contain OFCOM's conclusions and any recommendations for changes to the
regulations. This power allows the Secretary of State to make new regulations in
response to OFCOM’s report and other changes in the online environment.
Justification for the procedure

By virtue of clause 132(6)(b), on the first occasion of their use regulations made
under clause 45 must be approved by Parliament through the affirmative procedure.
The first use of this power will determine which descriptions of content that is harmful
to children providers of regulated services must prioritise, which will be a central
element of the new regulatory framework. It is therefore considered appropriate that
Parliament should be able to debate and approve the original regulations made under
this power.
By virtue of clause 132(5)(b), in all subsequent cases the power will be exercised
through the negative procedure. The negative procedure is considered to provide an
appropriate level of parliamentary scrutiny for regulations made under this power,
which will simply specify new descriptions of priority content or remove others from
the list. It will also allow the government to respond quickly to the emergence of
significant new types of harmful content.
Clause 46(2): Meaning of “content that is harmful to adults” etc

Power conferred on: Secretary of State
Power exercised by: Regulations
Parliamentary procedure: First time - affirmative procedure. Negative procedure
thereafter.

Context and purpose

Providers of Category 1 user-to-user services will have duties to protect adults from
legal but harmful content. They will have to carry out an adults’ risk assessment under
clause 7(5) and, under clause 11, to set out how they will deal with content that is
harmful to adults in their terms of service and apply those terms of service
consistently and transparently.
Clause 46 defines the meaning of “content that is harmful to adults” and confers a
power on the Secretary of State, under clause 46(2)(b)(i), to specify in regulations
priority categories of content harmful to adults. Service providers will have to
consider each of these priority categories of content in their risk assessments.
Justification for the power

This power is delegated to reflect the need to ensure that the regime is future-proofed
and will ensure new categories of priority content can be added, or existing categories
of priority content removed, in response to changes in risk or technologies.
This power is delegated to reflect the need to ensure that the regime is future-
proofed, but also to provide legal certainty. There will be parliamentary oversight of
the definition of content that is harmful to adults in the primary legislation, and then
parliamentary oversight of the specific categories of priority content that is harmful to
adults as part of the scrutiny of secondary legislation.
This delegated power will further ensure that the regulatory framework benefits fully
from OFCOM's expertise and research capacity. Under clause 47, the Secretary of
State must consult OFCOM before making these regulations. As with the clause 45(2)
power, within three years of the date of the first use of either the clause 45 or clause
46 power, OFCOM must prepare a report focusing on the incidence on regulated
services of content that is harmful to children and content that is harmful to adults,
and the severity of harm that individuals suffer, or may suffer, as a result of those
kinds of content. The report must contain OFCOM's conclusions and any
recommendations for changes to the regulations. This power allows the Secretary of
State to make new regulations in response to OFCOM’s report or other changes in
the online environment.
Justification for the procedure

By virtue of clause 132(6)(c), on the first occasion of their use regulations made under
clause 46 must be approved by Parliament through the affirmative procedure. The
first use of this power will determine which descriptions of content that is harmful to
adults providers of regulated services must prioritise, which will be a central element
of the new regulatory framework. It is therefore considered appropriate that
Parliament should be able to debate and approve the original regulations made under
this power.
By virtue of clause 132(5)(c), in all subsequent cases the power will be exercised
through the negative procedure. The negative procedure is considered to provide an
appropriate level of parliamentary scrutiny for regulations made under this power
which will simply specify new descriptions of priority content or remove others from
the list. It will also allow the government to respond quickly to the emergence of
significant new types of harmful content.
